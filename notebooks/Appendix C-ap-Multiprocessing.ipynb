{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B\n",
    "# Multiprocessing with encrypted data\n",
    "Author:\n",
    "\n",
    "Arthur Pignet - apignet@equancy.com\n",
    "\n",
    "As we use the CKKS encryption scheme, computations on ciphertext are really slow. In order to increase the speed of the training phase, we want to benefit from multiple cores. This notebook aims to demonstrate the feasibility of such tasks, and suggest an implementation.\n",
    "\n",
    "The main issue relies on providing the data to the processes. The data needs to be serializable, which is not really the case of the tenseal context and vectors. \n",
    "\n",
    "### Classic idea of multiprocessing\n",
    "\n",
    "Let say we dispose of N cores. To optimize the use of our cores, we want to create N subprocesses.\n",
    "For each sub-process, Linux creates (forks) a child process, with its own and closed memory space, as a copy of the parent process. So in theory, the children has a copy of all variables of the parent in its own memory. To save memory space, Linux use a system of copy-on-write. The principe is that the child process sees in its virtual memory all the variables that were in the memory space of parent process when the children was created. But Linux does copy the variable from the parent's memory to the child's memory only the first time the children try to write on the variable.\n",
    "With tenseal, CKKSvector and context cannot be copied with this principle, because the \"copy\" use the pickle*\n",
    "serialization, which is not allowed on these tenseal objects.\n",
    "\n",
    "### TenSEAL serialization\n",
    "\n",
    "We are interested in serialize two tenseal objects, context and CKKSvector. The context is the main tenseal object. When created, it holds the secret and public keys. So it is needed to encrypt a vector.We can add to this context the relin and Galois keys. These keys are mandatory for every computation, therefore every vector encrypted with a specific context holds as an attribute a pointer on the context. Moreover,if the context is private (ie holds the secret key), the method decrypt() on a vector will not need the secret key, as it can be accessed with vector.context.secret_key.\n",
    "It really convenient for computations but raise on issue on serialization. We we try to pickle a vector, the pickle module will try to serialize every attribute of the vector, so it will serialize, for every vector, a copy of the context, which is really memory consuming. to tackle this issue, the tenseal vectors and context came with their own serialization methods, build on top of Google protocol buffers.\n",
    "So while being serialize, CKKS vector drops its context. Therefore, to deserialize a CKKS vector, we need a copy of its native context.\n",
    "\n",
    "All in all, the protocol to transfer CKKS vectors can be written as follow :\n",
    "- serialize the context\n",
    "- serialize the vectors\n",
    "- send the binary (ie serialized ) vectors and context\n",
    "- deserialize the context\n",
    "- deserialize the vectors using the deserialized context\n",
    "\n",
    "### Multiprocessing implementation\n",
    "\n",
    "The full multiprocessing protocol, which will be tested below, can be described :\n",
    "\n",
    "We create N processes.\n",
    "We initializes these N processes by passing a function initialization(), which takes as parameters the context in binary, and 1/N of the data, in binary. The initialization() function creates global variables for the context and the dataset, calling the tenseal.context_from() and tenseal.ckks_vector_from() methods. As the initialization() function is executed locally by the process, the global variables are created kind of locally in the process memory. Then, the main process puts in input_queues the function to perform, with its parameters, which need to be pickle-serializable. FOr our case, the function will be batch-forward-backward-propagation, and the parameters the weights. Therefore the weights will be serialized. Each Process deserializes the weights, using its local context, computes the gradient on its local dataset, serializes it, and puts the serialized gradient to a queue to give it back to the parent process. The parent process will then (first deserialize the gradients) sums the partial gradients to get the final gradient, then computes the descent direction, so as to eventually actualize the weights and bias.\n",
    "We make the choice to copy the context N times, so as to give to every worker its own set of keys, which are mandatory for computation.\n",
    "The context would also have been stored in a shared memory, to save memory space. But as every worker needs to access the keys every time they compute something, it would have probably raised concurrent access issues.\n",
    "\n",
    "For the present demonstration, we will only make the sum of the whole dataset.\n",
    "\n",
    "We use the native module of python for multiprocessing, [multiprocessing](https://docs.python.org/fr/3/library/multiprocessing.html), which use the pickle module to serialize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import multiprocessing\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tenseal as ts\n",
    "\n",
    "os.chdir(\"/home/apignet/homomorphic-encryption/ckks_titanic/\")\n",
    "from src.features import build_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of parameters\n",
    "\n",
    "N is the number of worker. The work will be split in N blocks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = \"/home/apignet/homomorphic-encryption/ckks_titanic/\"\n",
    "DATA_PATH = WORKING_DIR+\"/data/raw/\"  \n",
    "SUBMISSION_PATH = WORKING_DIR+\"/report/submission/\"# whole data set\n",
    "#DATA_PATH = WORKING_DIR+\"/data/quick_demo/\"   # subset of the data set, with 15 train_samples and 5 test_samples\n",
    "\n",
    "LOG_PATH = WORKING_DIR+\"/reports/log/\"\n",
    "LOG_FILENAME = \"test_multi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileHandler = logging.FileHandler(\"{0}/{1}.log\".format(LOG_PATH, LOG_FILENAME))\n",
    "streamHandler = logging.StreamHandler(sys.stdout)\n",
    "logging.basicConfig(format=\"%(asctime)s  [%(levelname)-8.8s]  %(message)s\", \n",
    "                    datefmt='%m/%d/%Y %I:%M:%S %p', \n",
    "                    level = logging.INFO, \n",
    "                    handlers=[fileHandler, streamHandler]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static functions\n",
    "\n",
    "To encrypt the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crytp_array(X, local_context):\n",
    "    \"\"\"\n",
    "    This function encrypt a list of vector\n",
    "    \n",
    "    :parameters \n",
    "    ------------\n",
    "    \n",
    "    :param X ; list of list, interpreted as list of vector to encrypt\n",
    "    :param local_context ; TenSEAL context object used to encrypt\n",
    "    \n",
    "    :returns\n",
    "    ------------\n",
    "    \n",
    "    list ; list of CKKS ciphertext  \n",
    "    \n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(len(X)):\n",
    "        res.append(ts.ckks_vector(local_context, X[i]))\n",
    "        if i == len(X) // 4:\n",
    "            logging.info(\"25 % ...\")\n",
    "        elif i == len(X) // 2 :\n",
    "            logging.info(\"50 % ...\")\n",
    "        elif i == 3* len(X)//4:\n",
    "            logging.info(\"75% ...\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and processing the data\n",
    "\n",
    "As said, the detailed of the data processing, feature engineering can be found in the notebook Appendix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2020 03:36:06 PM  [INFO    ]  /home/apignet/homomorphic-encryption/ckks_titanic\n",
      "08/18/2020 03:36:06 PM  [INFO    ]  loading the data into memory (pandas df)\n",
      "08/18/2020 03:36:06 PM  [INFO    ]  Done\n",
      "08/18/2020 03:36:06 PM  [INFO    ]  making final data set from raw data\n",
      "08/18/2020 03:36:06 PM  [INFO    ]  Done\n",
      "08/18/2020 03:36:06 PM  [INFO    ]  /home/apignet/homomorphic-encryption/ckks_titanic\n",
      "08/18/2020 03:36:06 PM  [INFO    ]  loading the data into memory (pandas df)\n",
      "08/18/2020 03:36:06 PM  [INFO    ]  Done\n",
      "08/18/2020 03:36:06 PM  [INFO    ]  making final data set from raw data\n",
      "08/18/2020 03:36:06 PM  [INFO    ]  Done\n",
      "peak memory: 146.06 MiB, increment: 13.70 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "logging.info(os.getcwd())\n",
    "raw_train, raw_test = build_features.data_import(DATA_PATH)\n",
    "train, submission_test = build_features.processing(raw_train, raw_test)\n",
    "#del submission_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 146.31 MiB, increment: 0.01 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "train_labels = train.Survived\n",
    "train_features = train.drop(\"Survived\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of safety parameters\n",
    "\n",
    "See Notebooks 0 or 1 for more detail about the choice of those. \n",
    "\n",
    "Here we use the same parameters as in the real case, but it is only for evaluating the true memory size of the dataset and the context. In fact the operation performed (sum) requires less restrictive parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2020 03:36:07 PM  [INFO    ]  Definition of safety parameters...\n",
      "08/18/2020 03:36:07 PM  [INFO    ]  Done. 0.31 seconds\n",
      "08/18/2020 03:36:07 PM  [INFO    ]  Generation of the secret key...\n",
      "08/18/2020 03:36:07 PM  [INFO    ]  Done. 0.0 seconds\n",
      "08/18/2020 03:36:07 PM  [INFO    ]  Generation of the Galois Key...\n",
      "08/18/2020 03:36:10 PM  [INFO    ]  Done. 3.3 seconds\n",
      "08/18/2020 03:36:10 PM  [INFO    ]  Generation of the Relin Key...\n",
      "08/18/2020 03:36:10 PM  [INFO    ]  Done. 0.13 seconds\n",
      "08/18/2020 03:36:10 PM  [INFO    ]  The context is now public, the context do not hold the secret key anymore, and decrypt     methods need the secret key to be provide,\n",
      "08/18/2020 03:36:10 PM  [INFO    ]  serialization of the context started...\n",
      "08/18/2020 03:37:00 PM  [INFO    ]  Serialization done in 49.10720896720886 seconds.\n",
      "peak memory: 1572.49 MiB, increment: 1426.18 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "logging.info('Definition of safety parameters...')\n",
    "timer = time.time()\n",
    "# context = ts.context(ts.SCHEME_TYPE.CKKS, 32768,\n",
    "# coeff_mod_bit_sizes=[60, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 60])\n",
    "#context = ts.context(ts.SCHEME_TYPE.CKKS, 8192, coeff_mod_bit_sizes=[40, 21, 21, 21, 21, 21, 21, 40])\n",
    "\n",
    "context = ts.context(ts.SCHEME_TYPE.CKKS, 16384, coeff_mod_bit_sizes=[60, 40, 40, 40, 40, 40, 40,40, 60])\n",
    "context.global_scale = pow(2, 40)\n",
    "logging.info(\"Done. \" + str(round(time.time() - timer, 2)) + \" seconds\")\n",
    "\n",
    "\n",
    "logging.info('Generation of the secret key...')\n",
    "timer = time.time()\n",
    "secret_key = context.secret_key()\n",
    "context.make_context_public() #drop the relin keys, the galois keys, and the secret keys. \n",
    "logging.info(\"Done. \" + str(round(time.time() - timer, 2)) + \" seconds\")\n",
    "logging.info('Generation of the Galois Key...')\n",
    "timer = time.time()\n",
    "context.generate_galois_keys(secret_key)\n",
    "logging.info(\"Done. \" + str(round(time.time() - timer, 2)) + \" seconds\")\n",
    "logging.info('Generation of the Relin Key...')\n",
    "timer = time.time()\n",
    "context.generate_relin_keys(secret_key)\n",
    "logging.info(\"Done. \" + str(round(time.time() - timer, 2)) + \" seconds\")\n",
    "if context.is_public():\n",
    "    logging.info(\"The context is now public, the context do not hold the secret key anymore, and decrypt \\\n",
    "    methods need the secret key to be provide,\")\n",
    "logging.info(\"serialization of the context started...\")\n",
    "timer = time.time()\n",
    "b_context = context.serialize()\n",
    "logging.info(\"Serialization done in %s seconds.\" %(time.time()-timer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data encryption\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2020 03:37:00 PM  [INFO    ]  Data encryption...\n",
      "08/18/2020 03:37:05 PM  [INFO    ]  25 % ...\n",
      "08/18/2020 03:37:10 PM  [INFO    ]  50 % ...\n",
      "08/18/2020 03:37:15 PM  [INFO    ]  75% ...\n",
      "08/18/2020 03:37:25 PM  [INFO    ]  25 % ...\n",
      "08/18/2020 03:37:30 PM  [INFO    ]  50 % ...\n",
      "08/18/2020 03:37:35 PM  [INFO    ]  75% ...\n",
      "08/18/2020 03:37:40 PM  [INFO    ]  Done. 40.55 seconds\n",
      "peak memory: 4679.26 MiB, increment: 3562.12 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "logging.info(\"Data encryption...\")\n",
    "timer = time.time()\n",
    "encrypted_X = crytp_array(train_features.to_numpy().tolist(), context)\n",
    "encrypted_Y = crytp_array(train_labels.to_numpy().reshape((-1, 1)).tolist(), context)\n",
    "logging.info(\"Done. \" + str(round(time.time() - timer, 2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worker functions\n",
    "\n",
    "Definition of the function that will be performed continuously by the subprocess.\n",
    "\n",
    "Definition of the function that will initialize the subprocess, by creating in its own memory space the context and the subprocess sub-dataset.\n",
    "\n",
    "Definition of the function addition, that will be executed by the worker on its own partial dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(input_queue, output_queue):\n",
    "    \"\"\"\n",
    "    This functions turns on the process until a string 'STOP' is found in the input_queue queue\n",
    "\n",
    "    It takes every couple (function, arguments of the functions) from the input_queue queue, and put the result into the output queue\n",
    "    \"\"\"\n",
    "    for func, args in iter(input_queue.get, 'STOP'):\n",
    "        result = func(*args)\n",
    "        output_queue.put(result)\n",
    "\n",
    "        \n",
    "def initialization(*args):\n",
    "    \"\"\"\n",
    "    :param:tuple : (b_context, b_X)\n",
    "            b_context: binary representation of the context_. context_.serialize()$\n",
    "            b_X : list of binary representations of samples from CKKS vectors format\n",
    "    This function is the first one to be passed in the input_queue queue of the process.\n",
    "    It first deserialize the context_, passing it global,\n",
    "    in the memory space allocated to the process\n",
    "    Then the batch is also deserialize, using the context_,\n",
    "    to generate a list of CKKS vector which stand for the encrypted samples on which the process will work\n",
    "    \"\"\"\n",
    "    b_context = args[0]\n",
    "    b_X = args[1]\n",
    "    global context_\n",
    "    context_ = ts.context_from(b_context)\n",
    "    global local_X_\n",
    "    local_X_ = [ts.ckks_vector_from(context_, i) for i in b_X]\n",
    "    return 'Initialization done for process %s. Len of data : %i' % (\n",
    "        multiprocessing.current_process().name, len(local_X_))\n",
    "\n",
    "\n",
    "def addition(*arg):\n",
    "    res=0\n",
    "    for i in local_X_:\n",
    "        res = i + res \n",
    "    return res.serialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum without multiprocessing\n",
    "\n",
    "We first try this out without multiprocessing, on one core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2020 03:37:42 PM  [INFO    ]  sum performed in 1.3563103675842285 without multiprocessing\n"
     ]
    }
   ],
   "source": [
    "timer =time.time()\n",
    "res = 0 \n",
    "for x in encrypted_X:\n",
    "    res = x + res\n",
    "wct = time.time() -timer\n",
    "logging.info('sum performed in %s without multiprocessing' %(time.time()-timer\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum with multiprocessing. \n",
    "\n",
    "We first split the dataset into N batches. During the same time we serialize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2020 03:37:42 PM  [INFO    ]  Starting serialization of data\n",
      "08/18/2020 03:40:54 PM  [INFO    ]  Data serialization done in 191.8620753288269 seconds\n"
     ]
    }
   ],
   "source": [
    "timer = time.time()\n",
    "b_X = [[] for _ in range(N)]\n",
    "logging.info(\"Starting serialization of data\")\n",
    "ser_time = time.time()\n",
    "for i in range(len(encrypted_X)):\n",
    "    b_X[i % N].append(encrypted_X[i].serialize())\n",
    "ser_time = time.time() -timer\n",
    "logging.info(\"Data serialization done in %s seconds\" % (ser_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the module multiprocessing to create the N workers, the N in_queue and the queue_out\n",
    "As soon as a worker start, the initialization begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2020 03:40:54 PM  [INFO    ]  Initialization of 8 workers\n",
      "08/18/2020 03:41:01 PM  [INFO    ]  Initialization done for process Process-8. Len of data : 112\n",
      "08/18/2020 03:41:02 PM  [INFO    ]  Initialization done for process Process-9. Len of data : 112\n",
      "08/18/2020 03:41:02 PM  [INFO    ]  Initialization done for process Process-11. Len of data : 111\n",
      "08/18/2020 03:41:02 PM  [INFO    ]  Initialization done for process Process-10. Len of data : 112\n",
      "08/18/2020 03:41:02 PM  [INFO    ]  Initialization done for process Process-13. Len of data : 111\n",
      "08/18/2020 03:41:03 PM  [INFO    ]  Initialization done for process Process-12. Len of data : 111\n",
      "08/18/2020 03:41:03 PM  [INFO    ]  Initialization done for process Process-14. Len of data : 111\n",
      "08/18/2020 03:41:03 PM  [INFO    ]  Initialization done for process Process-15. Len of data : 111\n",
      "08/18/2020 03:41:03 PM  [INFO    ]  Initialization done in 9.765893459320068 seconds\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Initialization of %d workers\" % N)\n",
    "init_work_timer = time.time()\n",
    "list_queue_in = []\n",
    "queue_out = multiprocessing.Queue()\n",
    "init_tasks = [(initialization, (b_context, x)) for x in b_X]\n",
    "for init in init_tasks:\n",
    "    list_queue_in.append(multiprocessing.Queue())\n",
    "    list_queue_in[-1].put(init)\n",
    "    multiprocessing.Process(target=worker, args=(list_queue_in[-1], queue_out)).start()\n",
    "log_out = []\n",
    "for _ in range(N):\n",
    "    log_out.append(queue_out.get())\n",
    "    logging.info(log_out[-1])\n",
    "    \n",
    "init_time =time.time() - init_work_timer\n",
    "logging.info(\"Initialization done in %s seconds\" % (init_time))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, we now have our N workers which are listening for tasks. They all have their local context and data.\n",
    "\n",
    "We can now put the task in the input_queues and get the result back in the output queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2020 03:41:04 PM  [INFO    ]  Computation done in 1.0061945915222168 seconds\n"
     ]
    }
   ],
   "source": [
    "timer = time.time()\n",
    "for q in list_queue_in:\n",
    "    q.put((addition, ()))\n",
    "result = 0\n",
    "for _ in range(N):\n",
    "    child_process_ans = queue_out.get()\n",
    "    result = ts.ckks_vector_from(context, child_process_ans) + result\n",
    "computation_time = time.time() -timer\n",
    "for q in list_queue_in:\n",
    "    q.put('STOP')\n",
    "\n",
    "logging.info(\"Computation done in %s seconds\" % (computation_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of results\n",
    "\n",
    "When performing such quick operation, the multiprocessing is not worth it. The task of initialization, due to large amount of serialization performed, slows down the overall process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2020 03:41:04 PM  [INFO    ]  Without multiprocessing : 1.3562681674957275 seconds\n",
      "08/18/2020 03:41:04 PM  [INFO    ]  With multiprocessing, computatiton: 1.0061945915222168 seconds\n",
      "08/18/2020 03:41:04 PM  [INFO    ]  With multiprocessing, serialization: 191.8620753288269 seconds\n",
      "08/18/2020 03:41:04 PM  [INFO    ]  With multiprocessing, initialization: 9.765893459320068 seconds\n",
      "08/18/2020 03:41:04 PM  [INFO    ]  With multiprocessing, Overall: 202.6341633796692 seconds\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Without multiprocessing : %s seconds\"% wct)\n",
    "logging.info(\"With multiprocessing, computatiton: %s seconds\"% computation_time)\n",
    "logging.info(\"With multiprocessing, serialization: %s seconds\"% ser_time)\n",
    "logging.info(\"With multiprocessing, initialization: %s seconds\"% init_time)\n",
    "logging.info(\"With multiprocessing, Overall: %s seconds\"% (ser_time+init_time+computation_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But at the end the result is the same. In a real scenario, with a time consuming operation (gradient computations) the time gain is about the number of processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(np.array(res.decrypt(secret_key))-np.array(result.decrypt(secret_key))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}