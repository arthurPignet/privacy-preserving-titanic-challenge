{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import tenseal as ts\n",
    "import pickle as pk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score, f1_score, recall_score, precision_score, accuracy_score, classification_report\n",
    "os.chdir(\"/home/apignet/homomorphic-encryption/ckks_titanic/\")\n",
    "from src.features import build_features\n",
    "from models import encrypted_LR\n",
    "from models import unencrypted_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# definition of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = \"/data/raw/\"            # whole data set\n",
    "DATA_PATH = \"/data/quick_demo/\"   # subset of the data set, with 15 train_samples and 5 test_samples\n",
    "#DATA_PATH = \"/home/apignet/homomorphic-encryption/ckks_titanic/data/quick_demo/\"   # subset of the data set, with 400 train_samples and 50 test_samples\n",
    "#DATA_PATH =   '/data/quick_demo/'\n",
    "LOG_PATH = \"reports/log\"\n",
    "LOG_FILENAME = \"test_0716\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileHandler = logging.FileHandler(\"{0}/{1}.log\".format(LOG_PATH, LOG_FILENAME))\n",
    "streamHandler = logging.StreamHandler(sys.stdout)\n",
    "logging.basicConfig(format=\"%(asctime)s  [%(levelname)-8.8s]  %(message)s\", datefmt='%m/%d/%Y %I:%M:%S %p', level = logging.INFO, handlers=[fileHandler, streamHandler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/apignet/homomorphic-encryption/ckks_titanic'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 40\n",
    "LEARNING_RATE = 0.9\n",
    "MOMENTUM_RATE = 0.6\n",
    "REGULARIZATION_RATE = 0.5\n",
    "VERBOSE = 2\n",
    "SAVE_WEIGHT = 2\n",
    "N_JOBS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crytp_array(X, local_context):\n",
    "    \"\"\"\n",
    "    This function encrypt a list of vector\n",
    "    \n",
    "    :parameters \n",
    "    ------------\n",
    "    \n",
    "    :param X ; list of list, interpreted as list of vector to encrypt\n",
    "    :param local_context ; TenSEAL context object used to encrypt\n",
    "    \n",
    "    :returns\n",
    "    ------------\n",
    "    \n",
    "    list ; list of CKKS ciphertext  \n",
    "    \n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(len(X)):\n",
    "        res.append(ts.ckks_vector(local_context, X[i]))\n",
    "        if i == len(X) // 4:\n",
    "            logging.info(\"25 % ...\")\n",
    "        elif i == len(X) // 2 :\n",
    "            logging.info(\"50 % ...\")\n",
    "        elif i == 3* len(X)//4:\n",
    "            logging.info(\"75% ...\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidential functions\n",
    "\n",
    "These functions involves security breachs (as use of unencrypted data, or decryption of weights) and cannot be coded by Alice.\n",
    "However, the functions encapslulate the unsafe process, so can be performed by Alice if Bob provides them. \n",
    "Therefore, they are currently passed as parameters to Alice, which only calls them.\n",
    "\n",
    "Currently there is a huge security breach, as confidential parameters (security key for instance), which are needed by those functions, are passed in a dictionnary to Alice. \n",
    "For a safe protocole, we have to change these functions, to set up a safe communication protocole between Bob and Alice.\n",
    "Alice will therefore only send the crypted data to Bob (using these functions, in which can be set up the communication process) and Bob will locally perform the functions which are currently coded bellow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/30/2020 04:28:42 PM  [INFO    ]  /home/apignet/homomorphic-encryption/ckks_titanic\n",
      "07/30/2020 04:28:42 PM  [INFO    ]  loading the data into memory (pandas df)\n",
      "07/30/2020 04:28:42 PM  [INFO    ]  Done\n",
      "07/30/2020 04:28:42 PM  [INFO    ]  making final data set from raw data\n",
      "07/30/2020 04:28:42 PM  [INFO    ]  Done\n",
      "07/30/2020 04:28:42 PM  [INFO    ]  /home/apignet/homomorphic-encryption/ckks_titanic\n",
      "07/30/2020 04:28:42 PM  [INFO    ]  loading the data into memory (pandas df)\n",
      "07/30/2020 04:28:42 PM  [INFO    ]  Done\n",
      "07/30/2020 04:28:42 PM  [INFO    ]  making final data set from raw data\n",
      "07/30/2020 04:28:43 PM  [INFO    ]  Done\n",
      "peak memory: 162.49 MiB, increment: 13.83 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "logging.info(os.getcwd())\n",
    "raw_train, raw_test = build_features.data_import(os.getcwd()+DATA_PATH)\n",
    "train, submission_test = build_features.processing(raw_train, raw_test)\n",
    "del submission_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 162.81 MiB, increment: 0.08 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "train, test = train_test_split(train, test_size=0.15)\n",
    "train_labels = train.Survived\n",
    "test_labels = test.Survived\n",
    "train_features = train.drop(\"Survived\", axis=1)\n",
    "test_features = test.drop(\"Survived\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of safety parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/30/2020 04:28:44 PM  [INFO    ]  Definition of safety parameters...\n",
      "07/30/2020 04:28:44 PM  [INFO    ]  Done. 0.03 seconds\n",
      "07/30/2020 04:28:44 PM  [INFO    ]  Generation of the secret key...\n",
      "07/30/2020 04:28:44 PM  [INFO    ]  Done. 0.0 seconds\n",
      "07/30/2020 04:28:44 PM  [INFO    ]  Generation of the Galois Key...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Done. 0.11 seconds\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Generation of the Relin Key...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Done. 0.01 seconds\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  The context is now public, the context do not hold the secret key anymore, and decrypt methods need the secret key to be provide,\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Definition of safety parameters...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Done. 0.03 seconds\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Generation of the secret key...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Done. 0.0 seconds\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Generation of the Galois Key...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Done. 0.12 seconds\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Generation of the Relin Key...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Done. 0.01 seconds\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  The context is now public, the context do not hold the secret key anymore, and decrypt methods need the secret key to be provide,\n",
      "peak memory: 178.52 MiB, increment: 15.71 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "logging.info('Definition of safety parameters...')\n",
    "timer = time.time()\n",
    "# context = ts.context(ts.SCHEME_TYPE.CKKS, 32768,\n",
    "# coeff_mod_bit_sizes=[60, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 60])\n",
    "#context = ts.context(ts.SCHEME_TYPE.CKKS, 8192, coeff_mod_bit_sizes=[40, 21, 21, 21, 21, 21, 21, 40])\n",
    "\n",
    "context = ts.context(ts.SCHEME_TYPE.CKKS, 4096, coeff_mod_bit_sizes=[40, 20, 40])\n",
    "#context = ts.context(ts.SCHEME_TYPE.CKKS, 16384, coeff_mod_bit_sizes=[60, 40, 40, 40, 40, 40, 40,40, 60])\n",
    "context.global_scale = pow(2, 20)\n",
    "logging.info(\"Done. \" + str(round(time.time() - timer, 2)) + \" seconds\")\n",
    "\n",
    "\n",
    "logging.info('Generation of the secret key...')\n",
    "timer = time.time()\n",
    "secret_key = context.secret_key()\n",
    "context.make_context_public() #drop the relin keys, the galois keys, and the secret keys. \n",
    "logging.info(\"Done. \" + str(round(time.time() - timer, 2)) + \" seconds\")\n",
    "logging.info('Generation of the Galois Key...')\n",
    "timer = time.time()\n",
    "context.generate_galois_keys(secret_key)\n",
    "logging.info(\"Done. \" + str(round(time.time() - timer, 2)) + \" seconds\")\n",
    "logging.info('Generation of the Relin Key...')\n",
    "timer = time.time()\n",
    "context.generate_relin_keys(secret_key)\n",
    "logging.info(\"Done. \" + str(round(time.time() - timer, 2)) + \" seconds\")\n",
    "if context.is_public():\n",
    "    logging.info(\"The context is now public, the context do not hold the secret key anymore, and decrypt methods need the secret key to be provide,\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/30/2020 04:28:45 PM  [INFO    ]  Data encryption...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  25 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  50 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  75% ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  25 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  50 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  75% ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  25 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  50 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  25 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  50 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Done. 0.11 seconds\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Data encryption...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  25 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  50 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  75% ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  25 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  50 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  75% ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  25 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  50 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  25 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  50 % ...\n",
      "07/30/2020 04:28:45 PM  [INFO    ]  Done. 0.11 seconds\n",
      "peak memory: 181.28 MiB, increment: 2.76 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "logging.info(\"Data encryption...\")\n",
    "timer = time.time()\n",
    "encrypted_X = crytp_array(train_features.to_numpy().tolist(), context)\n",
    "encrypted_Y = crytp_array(train_labels.to_numpy().reshape((-1, 1)).tolist(), context)\n",
    "encrypted_test_X = crytp_array(test_features.to_numpy().tolist(), context)\n",
    "encrypted_test_Y = crytp_array(test_labels.to_numpy().reshape((-1, 1)).tolist(), context)\n",
    "logging.info(\"Done. \" + str(round(time.time() - timer, 2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the weight\n",
    "\n",
    "The weights have to be crypted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 181.41 MiB, increment: 0.13 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "unencrypted_weight = np.random.normal(loc=0,\n",
    "                                      scale=0.2, size =(train_features.to_numpy().shape[1]))\n",
    "unencrypted_bias = np.random.random((1))\n",
    "\n",
    "weight = ts.ckks_vector(context, unencrypted_weight.tolist())\n",
    "bias = ts.ckks_vector(context, unencrypted_bias.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidential data as yet stored into a dictionnary, and will be used during the training only by functions which are passed as arguments to the fit methods. This encapsulation of sensitive data will allows us to ensure security during training later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "confidential_data = {'context':context,\n",
    "                     'secret_key':secret_key, \n",
    "                     'unencrypted_X':train_features.to_numpy(),\n",
    "                     'unencrypted_Y':train_labels.to_numpy().reshape((-1, 1)) \n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "b_context = context.serialize()\n",
    "b_X = [x.serialize() for x in encrypted_X]\n",
    "b_weight = weight.serialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing multiprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into batches, one for each processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "batches = [b_X[:len(b_X)//2] ,b_X[len(b_X)//2:]]\n",
    "for i in batches:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of all the functions needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(input, output):\n",
    "    \"\"\"\n",
    "    This functions turns on the process until a string 'STOP' is found in the input queue\n",
    "    \n",
    "    It takes every couple (function, arguments of the functions) from the input queue, and put the result into the output queue\n",
    "    \"\"\"\n",
    "    for func, args in iter(input.get, 'STOP'):\n",
    "        result = func(*args)\n",
    "        output.put(result)\n",
    "\n",
    "def initialization(b_context, batch):\n",
    "    \"\"\"\n",
    "    :param: b_context : binary representation of the context. context.serialize()$\n",
    "    :param: batch : list of binary representations of CKKS vector\n",
    "    This function is the first one to be passed in the input queue of the process. \n",
    "    It first deserialaze the context, passing it global,\n",
    "    in the memory sapce allocated to the process\n",
    "    Then the batch is also deserialize, using the context, \n",
    "    to generate a list of CKKS vector which stand for the encrypted samples on which the proces will work\n",
    "    \"\"\"\n",
    "    global context\n",
    "    context = ts.context_from(b_context)\n",
    "    global data\n",
    "    data = [ts.ckks_vector_from(context, i ) for i in batch]\n",
    "    return 'Initialization done for process %s. Len of data : %i' %(multiprocessing.current_process().name, len(data) )\n",
    "      \n",
    "def op(b_weight):\n",
    "    \"\"\"\n",
    "    Aims to represent the forward_backward_prop in test. \n",
    "    \"\"\"\n",
    "    w=ts.ckks_vector_from(context, b_weight)\n",
    "    res = w.dot(data[0])\n",
    "    for vec in data[1:]:\n",
    "        res += w.dot(vec)\n",
    "    return res.serialize()\n",
    "\n",
    "def data_test():\n",
    "    return len(data)\n",
    "\n",
    "def testpickle(sk):\n",
    "    data[0].decrypt(sk)\n",
    "    return \"j'ai recu une secret key...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 219.91 MiB, increment: 20.97 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "NUMBER_OF_PROCESSES = 2\n",
    "INIT_TASKS = [ (initialization, (b_context, batch )) for batch in batches]\n",
    "LIST_QUEUE_IN = []\n",
    "LIST_PROCESSES = []\n",
    "QUEUE_OUT = multiprocessing.Queue()\n",
    "for init_task in INIT_TASKS:\n",
    "    LIST_QUEUE_IN.append(multiprocessing.Queue())\n",
    "    LIST_QUEUE_IN[-1].put(init_task)\n",
    "    LIST_PROCESSES.append(multiprocessing.Process(target=worker, args=(LIST_QUEUE_IN[-1],QUEUE_OUT)).start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization done for process Process-18. Len of data : 4\n",
      "Initialization done for process Process-19. Len of data : 4\n"
     ]
    }
   ],
   "source": [
    "log_out = []\n",
    "for _ in range(NUMBER_OF_PROCESSES):\n",
    "    log_out.append(QUEUE_OUT.get())\n",
    "    print(log_out[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): #epoch\n",
    "    b_weight = weight.serialize()\n",
    "    print(\"epoch %i\" %i)\n",
    "    for q in LIST_QUEUE_IN:\n",
    "        q.put((op ,(b_weight,)))\n",
    "    temoin = 0\n",
    "    for batch in batches:\n",
    "        temp = weight.dot(ts.ckks_vector_from(context, batch[0]))\n",
    "        for vec in batch[1:]: \n",
    "            temp += weight.dot(ts.ckks_vector_from(context, vec))\n",
    "        temoin = temp + temoin\n",
    "    res = 0\n",
    "    for _ in range(NUMBER_OF_PROCESSES):\n",
    "        log_out.append(QUEUE_OUT.get())\n",
    "        res = ts.ckks_vector_from(context, log_out[-1]) +res\n",
    "    \n",
    "    res = ts.ckks_vector(context, res.decrypt(secret_key))\n",
    "    weight+=res\n",
    "    weight = ts.ckks_vector(context, weight.decrypt(secret_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(log_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in LIST_QUEUE_IN:\n",
    "    q.put((op ,(b_weight,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_QUEUE_IN[1].put((testpickle, (secret_key,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, 'piche'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(NUMBER_OF_PROCESSES):\n",
    "    QUEUE_OUT.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_predictions =  ( 1 for i in range(10))\n",
    "Y = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-2b5717d982a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menc_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0menc_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "res = 0\n",
    "for i in range(len(enc_predictions)):\n",
    "    res -= Y[i] * (enc_predictions[i])\n",
    "    print(res)\n",
    "    res -= (1 - Y[i]) *(1 - enc_predictions[i])\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-1\n",
      "-3\n",
      "-6\n",
      "-10\n",
      "-15\n",
      "-21\n",
      "-28\n",
      "-36\n",
      "-45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-45"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = 0\n",
    "for y,pred in zip(Y,enc_predictions):\n",
    "    res -= y * (pred)\n",
    "    print(res)\n",
    "    res -= (1 - y) *(1 - pred)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n",
      "2 2 2\n"
     ]
    }
   ],
   "source": [
    "res = 0\n",
    "for x,y,z in zip([1,2],[1,2],[1,2]):\n",
    "    print(x,y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
